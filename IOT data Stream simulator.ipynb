{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2fa915-0517-46ca-8477-246f7ccf371e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdown is not installed. Installing it now...\ngdown has been successfully installed.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkxgb is not installed. Installing it now...\nsparkxgb has been successfully installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import gdown\n",
    "    print(\"gdown is installed.\")\n",
    "except ImportError:\n",
    "    print(\"gdown is not installed. Installing it now...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.check_call([\"pip\", \"install\", \"gdown\"])\n",
    "        print(\"gdown has been successfully installed.\")\n",
    "        import gdown  # Importing again to use it after installation\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing gdown: {e}\")\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer, OneHotEncoder,StringIndexer\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from xgboost.spark import SparkXGBClassifier\n",
    "    print(\"sparkxgb is installed.\")\n",
    "except ImportError:\n",
    "    print(\"sparkxgb is not installed. Installing it now...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        command = \"pip install pyarrow pandas venv-pack xgboost\"\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        print(\"sparkxgb has been successfully installed.\")\n",
    "        from xgboost.spark import SparkXGBClassifier\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing sparkxgb: {e}\")\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer, OneHotEncoder,StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import heapq\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2306344-2e75-4a01-b4f5-b38ece1ea380",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clear_old_files(folder_path, keep_latest=10):\n",
    "    # List all files in the folder\n",
    "    all_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    # Sort files by modification time (most recent first)\n",
    "    all_files.sort(key=lambda f: os.path.getmtime(os.path.join(folder_path, f)), reverse=True)\n",
    "    \n",
    "    # Keep the latest files\n",
    "    files_to_keep = all_files[:keep_latest]\n",
    "    \n",
    "    # Delete older files\n",
    "    for file_name in all_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name not in files_to_keep:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d3c135-e39c-49de-a338-4f78d8240d00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'Trends_market_place_data': No such file or directory\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1SznHDvix3NdpmiIloyDxeAjiFtY-nk38\nFrom (redirected): https://drive.google.com/uc?id=1SznHDvix3NdpmiIloyDxeAjiFtY-nk38&confirm=t&uuid=8e88f336-f0f4-409a-83b9-2a71721ff372\nTo: /databricks/driver/Trends_market_place_data.zip\n\r  0%|          | 0.00/767M [00:00<?, ?B/s]\r  1%|          | 4.72M/767M [00:00<00:24, 30.7MB/s]\r  1%|          | 8.91M/767M [00:00<00:27, 27.1MB/s]\r  2%|▏         | 17.3M/767M [00:00<00:19, 38.0MB/s]\r  3%|▎         | 25.7M/767M [00:00<00:25, 28.7MB/s]\r  4%|▍         | 34.1M/767M [00:01<00:25, 28.4MB/s]\r  6%|▌         | 42.5M/767M [00:01<00:25, 28.8MB/s]\r  7%|▋         | 50.9M/767M [00:01<00:20, 34.3MB/s]\r  8%|▊         | 59.2M/767M [00:01<00:19, 36.0MB/s]\r  9%|▉         | 67.6M/767M [00:02<00:18, 38.0MB/s]\r 10%|▉         | 76.0M/767M [00:02<00:16, 41.8MB/s]\r 11%|█         | 84.4M/767M [00:02<00:16, 41.8MB/s]\r 12%|█▏        | 92.8M/767M [00:02<00:14, 45.8MB/s]\r 13%|█▎        | 101M/767M [00:02<00:17, 37.9MB/s] \r 14%|█▍        | 110M/767M [00:03<00:18, 36.2MB/s]\r 15%|█▌        | 118M/767M [00:03<00:16, 39.5MB/s]\r 16%|█▋        | 126M/767M [00:03<00:15, 41.7MB/s]\r 19%|█▊        | 143M/767M [00:03<00:10, 57.2MB/s]\r 20%|█▉        | 152M/767M [00:03<00:09, 61.6MB/s]\r 21%|██        | 160M/767M [00:03<00:09, 64.7MB/s]\r 23%|██▎       | 175M/767M [00:03<00:07, 82.7MB/s]\r 24%|██▍       | 184M/767M [00:04<00:10, 58.2MB/s]\r 25%|██▌       | 193M/767M [00:04<00:10, 53.6MB/s]\r 26%|██▋       | 202M/767M [00:04<00:09, 58.7MB/s]\r 27%|██▋       | 210M/767M [00:04<00:11, 49.6MB/s]\r 30%|██▉       | 227M/767M [00:04<00:09, 57.5MB/s]\r 31%|███       | 235M/767M [00:05<00:08, 59.5MB/s]\r 32%|███▏      | 247M/767M [00:05<00:07, 71.3MB/s]\r 33%|███▎      | 256M/767M [00:05<00:10, 50.8MB/s]\r 35%|███▌      | 269M/767M [00:05<00:10, 48.0MB/s]\r 37%|███▋      | 286M/767M [00:06<00:09, 50.4MB/s]\r 39%|███▉      | 303M/767M [00:06<00:07, 62.7MB/s]\r 41%|████      | 311M/767M [00:06<00:07, 62.0MB/s]\r 43%|████▎     | 328M/767M [00:06<00:05, 73.3MB/s]\r 45%|████▍     | 344M/767M [00:06<00:05, 77.9MB/s]\r 46%|████▌     | 353M/767M [00:06<00:06, 67.9MB/s]\r 48%|████▊     | 371M/767M [00:07<00:04, 88.4MB/s]\r 50%|████▉     | 381M/767M [00:07<00:04, 85.7MB/s]\r 51%|█████▏    | 395M/767M [00:07<00:04, 89.9MB/s]\r 54%|█████▍    | 416M/767M [00:07<00:02, 118MB/s] \r 56%|█████▌    | 430M/767M [00:07<00:03, 108MB/s]\r 58%|█████▊    | 445M/767M [00:07<00:03, 104MB/s]\r 61%|██████    | 465M/767M [00:07<00:02, 125MB/s]\r 62%|██████▏   | 479M/767M [00:07<00:02, 123MB/s]\r 65%|██████▍   | 495M/767M [00:08<00:02, 129MB/s]\r 67%|██████▋   | 515M/767M [00:08<00:01, 145MB/s]\r 69%|██████▉   | 533M/767M [00:08<00:01, 152MB/s]\r 72%|███████▏  | 549M/767M [00:08<00:01, 142MB/s]\r 74%|███████▍  | 570M/767M [00:08<00:01, 160MB/s]\r 77%|███████▋  | 588M/767M [00:08<00:01, 164MB/s]\r 79%|███████▉  | 605M/767M [00:08<00:01, 146MB/s]\r 81%|████████  | 621M/767M [00:08<00:01, 111MB/s]\r 83%|████████▎ | 638M/767M [00:09<00:01, 88.4MB/s]\r 86%|████████▌ | 659M/767M [00:09<00:00, 109MB/s] \r 88%|████████▊ | 672M/767M [00:09<00:01, 77.7MB/s]\r 90%|████████▉ | 688M/767M [00:09<00:00, 89.4MB/s]\r 91%|█████████▏| 700M/767M [00:09<00:00, 91.4MB/s]\r 93%|█████████▎| 712M/767M [00:10<00:00, 80.3MB/s]\r 95%|█████████▌| 730M/767M [00:10<00:00, 70.9MB/s]\r 98%|█████████▊| 750M/767M [00:10<00:00, 91.7MB/s]\r 99%|█████████▉| 762M/767M [00:10<00:00, 80.0MB/s]\r100%|██████████| 767M/767M [00:10<00:00, 71.0MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  Trends_market_place_data.zip\n   creating: Trends_market_place_data/\n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-35-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-35-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-3-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-3-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-9-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-9-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-21-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-21-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-34-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-34-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-60-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-60-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-44-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-44-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-20-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-20-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-48-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-48-1conn.log.labeled.csv  \n  inflating: Trends_market_place_data/CTU-IoT-Malware-Capture-42-1conn.log.labeled.csv  \n  inflating: __MACOSX/Trends_market_place_data/._CTU-IoT-Malware-Capture-42-1conn.log.labeled.csv  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -r Trends_market_place_data\n",
    "# Replace 'YOUR_FILE_ID' with the actual file ID from the Google Drive link\n",
    "file_id=\"1SznHDvix3NdpmiIloyDxeAjiFtY-nk38\"\n",
    "\n",
    "# Replace 'output_file.zip' with the desired output file name\n",
    "output_file=\"Trends_market_place_data.zip\"\n",
    "\n",
    "# Download the file from Google Drive using gdown\n",
    "gdown \"https://drive.google.com/uc?id=${file_id}\" -O \"${output_file}\"\n",
    "\n",
    "unzip Trends_market_place_data.zip\n",
    "rm -f Trends_market_place_data.zip\n",
    "rm -rf __MACOSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ead5c9-eaac-4a89-bb63-80f9525aa13d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf /databricks/driver/tmp \n",
    "mkdir /databricks/driver/tmp\n",
    "mkdir /databricks/driver/tmp/static\n",
    "rm -rf /databricks/driver/output \n",
    "mkdir /databricks/driver/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f2badc-4f3e-4c9d-875d-b5e3e1440d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#dbutils.fs.rm(\"dbfs:/databricks/driver/tmp\", recurse=True)\n",
    "#dbutils.fs.mkdirs(\"dbfs:/databricks/driver/tmp\")\n",
    "\n",
    "#dbutils.fs.rm(\"dbfs:/databricks/driver/output\", recurse=True)\n",
    "#dbutils.fs.mkdirs(\"dbfs:/databricks/driver/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97fa74f-1a1d-4cbe-8b3d-e29622a9b56c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_df(network_labelled_data):\n",
    "    #Processing data\n",
    "    columns_to_remove = [0, 2, 4, 12, 13, 14, 20, 22]\n",
    "    columns_to_drop = [network_labelled_data.columns[i] for i in columns_to_remove]\n",
    "\n",
    "    # Drop the specified columns\n",
    "    df_without_columns = network_labelled_data.drop(*columns_to_drop)\n",
    "\n",
    "    # Define the transformation using when\n",
    "    df_without_columns = df_without_columns.withColumn(\n",
    "        \"Malicious\", \n",
    "        when(col(\"label\") == \"Benign\", 0).otherwise(1)\n",
    "    )\n",
    "\n",
    "    # Drop the original label column if needed\n",
    "    df_without_columns = df_without_columns.drop(\"label\")\n",
    "\n",
    "    # Columns to transform\n",
    "    columns_to_transform = [4, 5, 6, 7, 9]\n",
    "    columns_to_transform = [df_without_columns.columns[i] for i in columns_to_transform]\n",
    "\n",
    "    # Columns to put 0 if '-'\n",
    "    zeros = [5, 6, 7]\n",
    "    zeros = [df_without_columns.columns[i] for i in zeros]\n",
    "\n",
    "    # Replace '-'\n",
    "    for column in zeros:\n",
    "        df_without_columns = df_without_columns.withColumn(\n",
    "            column,\n",
    "            when((col(column) == \"-\") , 0).otherwise(col(column)),\n",
    "        )\n",
    "\n",
    "    # Columns to convert to float\n",
    "    columns_to_convert_to_float = [5]\n",
    "    columns_to_convert_to_float = [df_without_columns.columns[i] for i in columns_to_convert_to_float]\n",
    "\n",
    "    for column in columns_to_convert_to_float:\n",
    "        df_without_columns = df_without_columns.withColumn(column, col(column).cast(FloatType()))\n",
    "\n",
    "    # Columns to convert to int\n",
    "    column_mapping_convert = [1, 2]\n",
    "    column_mapping_convert = [df_without_columns.columns[i] for i in column_mapping_convert]\n",
    "\n",
    "    # Define the mapping for column renaming\n",
    "    column_mapping = {\n",
    "        column_mapping_convert[0]: \"id_orig_p\",\n",
    "        column_mapping_convert[1]: \"id_resp_p\",\n",
    "    }\n",
    "\n",
    "    # Rename columns using withColumnRenamed\n",
    "    for old_col, new_col in column_mapping.items():\n",
    "        df_without_columns = df_without_columns.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "\n",
    "    # Columns to convert to int\n",
    "    columns_to_convert_to_int = [1, 2, 6, 7, 10, 11, 12]\n",
    "    columns_to_convert_to_int = [df_without_columns.columns[i] for i in columns_to_convert_to_int]\n",
    "\n",
    "\n",
    "    # Remove rows where the first column contains an IP address\n",
    "    df_without_columns = df_without_columns.filter(~col(columns_to_convert_to_int[1]).rlike(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"))\n",
    "\n",
    "    for column in columns_to_convert_to_int:\n",
    "        df_without_columns = df_without_columns.withColumn(column, col(column).cast(IntegerType()))\n",
    "    return df_without_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339dbc1c-b52f-4ef8-8fb1-668120bb7131",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/databricks/driver\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6681d6-ab52-4d29-9aa4-532fec2b754a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "# Specify the path to the folder containing CSV files\n",
    "csv_path = r'file:/databricks/driver/Trends_market_place_data/CTU-IoT-Malware-Capture-34-1conn.log.labeled.csv'\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"|\").option(\"inferschema\", \"true\").csv(csv_path)\n",
    "\n",
    "# Choose a random chunk of data (e.g., 10 rows)\n",
    "chunk_size = 10\n",
    "random_chunk = df.sample(False, chunk_size / df.count())\n",
    "random_chunk = process_df(random_chunk)\n",
    "# Specify the path to the folder where you want to save the random chunk\n",
    "output_folder = r'/databricks/driver/tmp/static/'\n",
    "\n",
    "# Specify the output file path\n",
    "output_file = output_folder+'random_chunk.csv'\n",
    "\n",
    "# Save the random chunk to a new CSV file\n",
    "#random_chunk.limit(1).write.csv(output_file, header=True, mode='overwrite')\n",
    "random_chunk.limit(1).toPandas().to_csv(output_file, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c34926-9aba-430f-8af9-5b5769e1b662",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>uid</th><th>id_orig_p</th><th>id_resp_p</th><th>proto</th><th>service</th><th>duration</th><th>orig_bytes</th><th>resp_bytes</th><th>conn_state</th><th>history</th><th>orig_pkts</th><th>orig_ip_bytes</th><th>resp_pkts</th><th>resp_ip_bytes</th><th>Malicious</th></tr></thead><tbody><tr><td>CxRJSN1zDC8dmbgdb3</td><td>49708</td><td>6667</td><td>tcp</td><td>-</td><td>0.0</td><td>0</td><td>0</td><td>S0</td><td>S</td><td>1</td><td>60</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "CxRJSN1zDC8dmbgdb3",
         49708,
         6667,
         "tcp",
         "-",
         0.0,
         0,
         0,
         "S0",
         "S",
         1,
         60,
         0,
         0,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "uid",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id_orig_p",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "id_resp_p",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "proto",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "service",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "duration",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "orig_bytes",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "resp_bytes",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "conn_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "history",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "orig_pkts",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "orig_ip_bytes",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "resp_pkts",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "resp_ip_bytes",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Malicious",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_chunk.limit(1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380eb774-cc29-4dc6-b44d-7a5114ab50fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round_1\nround_2\nround_3\nround_4\nround_5\nround_6\nround_7\nround_8\nround_9\nround_10\nround_11\nround_12\nround_13\nround_14\nround_15\nround_16\nround_17\nround_18\nround_19\nround_20\nround_21\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "count = 0\n",
    "\n",
    "# Specify the path to the folder containing CSV files\n",
    "csv_path = r'file:/databricks/driver/Trends_market_place_data/CTU-IoT-Malware-Capture-34-1conn.log.labeled.csv'\n",
    "while True:\n",
    "    count += 1\n",
    "    # Read the CSV file into a Spark DataFrame\n",
    "    df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"|\").option(\"inferschema\", \"true\").csv(csv_path)\n",
    "\n",
    "    # Choose a random chunk of data (e.g., 10 rows)\n",
    "    chunk_size = 10\n",
    "    random_chunk = df.sample(False, chunk_size / df.count())\n",
    "    random_chunk = process_df(random_chunk)\n",
    "    # Specify the path to the folder where you want to save the random chunk\n",
    "    output_folder = r'/databricks/driver/tmp/'\n",
    "\n",
    "    # Specify the output file path\n",
    "    output_file = output_folder+'random_chunk_{}.csv'.format(count)\n",
    "\n",
    "    # Save the random chunk to a new CSV file\n",
    "    #random_chunk.limit(1).write.csv(output_file, header=True, mode='overwrite')\n",
    "    random_chunk.limit(1).toPandas().to_csv(output_file, header=True)\n",
    "    #time.sleep(0.5)\n",
    "    #clear_old_files(r\"/databricks/driver/tmp/\", 10)\n",
    "    print( \"round_{}\".format(count))\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IOT data Stream simulator",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
